---
jupytext:
  cell_metadata_filter: -all
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.5
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
# Intro - Fake & Synthetic Data

[Synthetic](https://mostly.ai/blog/synthetic-data-generator-for-healthy-test-data/)
**Synthetic test data**  is generated by AI, that is trained on real data. It is structurally representative, referential integer data with support for relational structures. AI-generated synthetic data is not mock data or fake data. It is as much a representation of the behavior of your customers as production data. It’s not generated manually, but by a powerful AI engine that is capable of learning all the qualities of the dataset it is trained on, providing 100% test coverage. A good quality synthetic data generator can automate test data generation with high efficiency and without privacy concerns. Customer data should always be used in its synthetic form to protect privacy and to retain business rules embedded in the data. For example, mobile banking apps should be tested with synthetic transaction data, that is based on real customer transactions. 

TL;DR
Synthetic data generation methods changed significantly with the advance of AI
AI-generated, sample-based synthetic data is an entirely different beast than random or mock data
Types of AI-generated synthetic data include synthetic images, synthetic text, synthetic geolocation data, categorical, numerical and time-series data.
Stochastic processes are still useful if you care about data structure but not content
Rule-based systems can be used for simple use cases with low, fixed requirements toward complexity
Use deep generative models to automatically retain structure as well as information of data at scale to unlock private data and reduce model-to-market time
What the most common synthetic data types and their most common use cases are.
An overview of data generation methods
Not all synthetic dataset is created equal and in particular, synthetic data generation today is very different from what it was 5 years ago. Let’s take a look at different methods of synthetic data generation from the most rudimental forms to the state-of-the-art methods to see how far the technology has advanced! In this post we will distinguish between three major methods:

The stochastic process: random data is generated, only mimicking the structure of real data.
Rule-based data generation: mock data is generated following specific rules defined by humans.
Deep generative models: rich and realistic synthetic data is generated by a machine learning model trained on real data, replicating its structure and the information it contains.

# Definitions


SMOTE (Synthetic Minority Oversampling Technique) to deal with imbalanced classes in a dataset.

Another well-known technique is called SMOTE, which involves data augmentation (i.e., synthesizing new data samples) well before you use a clas- sification algorithm. SMOTE was initially developed by means of the kNN algorithm (other options are available), and it can be an effective technique for handling imbalanced classes.
Yet another option to consider is the Python package imbalanced-learn in the scikit-learn-contrib project. This project provides various re-sampling techniques for datasets that exhibit class imbalance. More details are available online:
https://github.com/scikit-learn-contrib/imbalanced-learn.
https://www.datprof.com/solutions/synthetic-test-data-generation/


https://mostly.ai/wp-content/uploads/2021/09/MOSTLY-AI_comparison_of_synthetic_data_types_2-1-1024x724.png


WHAT IS SMOTE?
SMOTE is a technique for synthesizing new samples for a dataset. This tech- nique is based on linear interpolation:
Step 1: Select samples that are close in the feature space.
Step 2: Draw a line between the samples in the feature space.
Step 3: Draw a new sample at a point along that line.
A more detailed explanation of the SMOTE algorithm is as follows:
Select a random sample “a” from the minority class.
Find k nearest neighbors for that example.
Select a random neighbor “b” from the nearest neighbors.
Create a line “L” that connects “a” and “b.”
Randomly select one or more points “c” on line L.
If need be, you can repeat this process for the other (k-1) nearest neigh- bors to distribute the synthetic values more evenly among the nearest neighbors.
SMOTE Extensions
The initial SMOTE algorithm is based on the kNN classification algorithm, which has been extended in various ways, such as replacing kNN with SVM. A list of SMOTE extensions is shown as follows:
selective synthetic sample generation
Borderline-SMOTE (kNN)
Borderline-SMOTE (SVM)
Adaptive Synthetic Sampling (ADASYN)
